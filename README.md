# Self Healing Pipelines on House Price Prediction

A production-ready machine learning pipeline for house price prediction with **self-healing MLOps capabilities**, featuring automated drift detection, performance monitoring, and intelligent retraining using MLflow, FastAPI, and Docker.

![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![MLflow](https://img.shields.io/badge/MLflow-2.8.1-orange)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-green)
![Docker](https://img.shields.io/badge/Docker-ready-blue)
![Evidently](https://img.shields.io/badge/Evidently-0.4.15-red)

---

## ğŸ¯ Features

- **End-to-End ML Pipeline**: Data ingestion, preprocessing, training & deployment
- **Experiment Tracking**: MLflow-based experiment management & model registry
- **REST API**: FastAPI prediction service with high performance
- **ğŸ”„ Self-Healing Pipeline**: Automated drift detection and model retraining
- **ğŸ“Š Real-time Monitoring**: Evidently AI for drift detection + Prometheus metrics
- **ğŸ¤– Automated Retraining**: Intelligent triggers based on drift and performance degradation
- **âœ… Model Validation**: Automated testing before production deployment
- **CI/CD**: Complete GitHub Actions workflows for monitoring and retraining
- **Containerized**: Docker-ready deployment with docker-compose support
- **Dashboard**: Real-time model performance tracking and drift reports

---

## ğŸ“ Project Structure

```bash
MLOPs-Project/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ main.yml                # ğŸ†• Complete pipeline (all stages)
â”‚   â”‚   â”œâ”€â”€ ci.yml                  # Continuous integration
â”‚   â”‚   â”œâ”€â”€ monitoring.yml          # Production monitoring (every 6h)
â”‚   â”‚   â”œâ”€â”€ retrain.yml             # Automated retraining pipeline
â”‚   â”‚   â””â”€â”€ retrain_deploy.yml      # Manual deployment
â”‚   â””â”€â”€ triggers/                   # Retraining trigger files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original datasets
â”‚   â”œâ”€â”€ processed/                  # Preprocessed data
â”‚   â”œâ”€â”€ reference/                  # Baseline data for drift detection
â”‚   â””â”€â”€ production/                 # Production data batches
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                       # Data ingestion & preprocessing
â”‚   â”œâ”€â”€ models/                     # Model training & evaluation
â”‚   â”œâ”€â”€ monitoring/                 # ğŸ†• Monitoring module
â”‚   â”‚   â”œâ”€â”€ drift_detector.py       # Evidently AI drift detection
â”‚   â”‚   â”œâ”€â”€ performance_monitor.py  # Performance metrics tracking
â”‚   â”‚   â””â”€â”€ monitoring_service.py   # Main monitoring orchestrator
â”‚   â””â”€â”€ api/                        # FastAPI application
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml                 # Main configuration
â”‚   â”œâ”€â”€ model_config.yaml           # Model hyperparameters
â”‚   â””â”€â”€ monitoring_config.json      # ğŸ†• Monitoring settings
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_monitoring.py           # ğŸ†• Manual monitoring execution
â”‚   â”œâ”€â”€ validate_model.py           # ğŸ†• Model validation tests
â”‚   â”œâ”€â”€ register_model.py           # ğŸ†• MLflow registry integration
â”‚   â”œâ”€â”€ promote_model.py            # ğŸ†• Production promotion
â”‚   â””â”€â”€ update_reference_data.py    # ğŸ†• Reference data management
â”œâ”€â”€ monitoring/                     # ğŸ†• Monitoring outputs
â”‚   â”œâ”€â”€ reports/                    # Drift HTML reports
â”‚   â”œâ”€â”€ metrics/                    # Performance metrics history
â”‚   â””â”€â”€ alerts/                     # Alert notifications
â”œâ”€â”€ docker/                         # Docker deployment files
â”œâ”€â”€ tests/                          # Unit & integration tests
â””â”€â”€ notebooks/                      # Jupyter notebooks
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
git clone <your-repo-url>
cd MLOPs-Project

# Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
pip install -r requirements-dev.txt
pip install -e .
```

### 2. Download Data

```bash
make download-data
# or
bash scripts/download_data.sh
```

### 3. Prepare Reference Data for Monitoring

```bash
# Create reference dataset from training data
mkdir -p data/reference
cp data/processed/train_data.csv data/reference/reference_data.csv
```

### 4. Train Model

```bash
make train
# or
python src/models/train.py
```

### 5. Start MLflow UI

```bash
make mlflow-ui
# Access at http://localhost:5000
```

### 6. Start API Server

```bash
make serve
# API available at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

---

## ğŸ”„ GitHub Actions Workflows

### Available Workflows

When you push to the repository, you'll see **4 workflows** in GitHub Actions:

#### 1. **ğŸš€ Complete MLOps Pipeline** (`main.yml`)
- **Trigger**: Push to `main` or `develop`, Pull Requests
- **Purpose**: Runs all stages sequentially
- **Stages**:
  1. ğŸ” **CI** - Lint, Test & Validate
  2. ğŸ¯ **Model Training** - Train with MLflow
  3. âœ… **Model Validation** - Performance checks
  4. ğŸ“Š **Monitoring Setup** - Initialize infrastructure
  5. ğŸ” **Monitoring Check** - Drift detection
  6. ğŸš€ **API Build & Test** - Test endpoints
  7. ğŸ³ **Docker Build** - Container creation
  8. ğŸ“‹ **Deployment Summary** - Final report

**Note**: All workflows use GitHub Actions artifact actions v4 for improved performance and reliability.

#### 2. **ğŸ” CI - Continuous Integration** (`ci.yml`)
- **Trigger**: Pull Requests, Manual
- **Purpose**: Multi-version Python testing
- **Tests**: Python 3.9, 3.10, 3.11

#### 3. **ğŸ“Š Monitoring - Production Model** (`monitoring.yml`)
- **Trigger**: Every 6 hours, Manual
- **Purpose**: Monitor production model performance
- **Actions**: Drift detection, performance tracking, alert generation

#### 4. **ğŸ”„ Retrain - Automated Retraining** (`retrain.yml`)
- **Trigger**: Manual, Repository dispatch from monitoring
- **Purpose**: Retrain and deploy new models
- **Stages**: Fetch data â†’ Train â†’ Validate â†’ Register â†’ Promote

### Workflow Execution on Push

When you push to `main` or `develop`:
- The **ğŸš€ Complete MLOps Pipeline** (`main.yml`) runs automatically.
- This pipeline includes all stages: CI, Model Training, Validation, Monitoring Setup, Monitoring Check, API Build & Test, Docker Build, and Deployment Summary.
- You can monitor the progress in the Actions tab of your GitHub repository.

---

## ğŸ”„ Self-Healing MLOps Pipeline

### Overview

The self-healing pipeline continuously monitors your model in production and automatically triggers retraining when performance degrades or data drift is detected, creating a **closed-loop autonomous system**.

### Architecture Components

#### 1. **Drift Detection** (Evidently AI)
- **Data Drift**: Monitors feature distribution changes
- **Target Drift**: Tracks prediction distribution shifts
- **Configurable Thresholds**: Customizable sensitivity
- **HTML Reports**: Visual drift analysis

#### 2. **Performance Monitoring** (Prometheus + Custom)
- **Key Metrics**: Accuracy, F1, Precision, Recall
- **Historical Tracking**: Time-series performance data
- **Degradation Detection**: Automatic alerts on performance drops
- **Prometheus Integration**: Optional pushgateway support

#### 3. **Automated Retraining**
- **Smart Triggers**: Based on drift + performance thresholds
- **Cooldown Period**: Prevents excessive retraining (24h default)
- **MLflow Integration**: Full experiment tracking
- **Model Registry**: Automatic versioning and staging
- **Validation Gates**: Quality checks before deployment

### Setup Monitoring

#### 1. Configure Monitoring Parameters

Edit `config/monitoring_config.json`:

```json
{
  "reference_data_path": "data/reference/reference_data.csv",
  "drift_threshold": 0.5,
  "performance_threshold": 0.75,
  "reports_dir": "monitoring/reports",
  "metrics_dir": "monitoring/metrics",
  "alerts_dir": "monitoring/alerts",
  "retraining_cooldown_hours": 24,
  "enable_auto_retrain": true,
  "monitoring_schedule": "0 */6 * * *",
  "batch_monitoring_enabled": true,
  "prometheus_gateway": null
}
```

#### 2. Initialize Monitoring Structure

```bash
mkdir -p monitoring/{reports,metrics,alerts}
mkdir -p data/{reference,production}
mkdir -p .github/triggers
```

### Running Monitoring

#### Manual Monitoring Check

```bash
python scripts/run_monitoring.py \
  --data-path data/production/current_batch.csv \
  --predictions-path data/production/predictions.csv \
  --labels-path data/production/labels.csv \
  --model-version v1.0.0
```

#### Automated Monitoring (GitHub Actions)
The monitoring workflow runs automatically every 6 hours via `.github/workflows/monitoring.yml`

### Retraining Pipeline

When monitoring detects issues, the automated retraining pipeline:

1. **Checks trigger conditions**
   - Data drift above threshold
   - Performance degradation detected
   - Cooldown period elapsed

2. **Fetches fresh data** and trains new model

3. **Validates new model** against test set

4. **Registers in MLflow** Model Registry

5. **Promotes to production** if validation passes

6. **Updates reference data** for future drift detection

### Monitoring Reports

- **Drift Reports**: `monitoring/reports/drift_report_*.html`
- **Performance Metrics**: `monitoring/metrics/metrics_*.json`
- **Alerts**: `monitoring/alerts/alert_*.json`

### CI/CD Integration

The self-healing pipeline integrates with GitHub Actions:

- `.github/workflows/monitoring.yml` - Scheduled monitoring
- `.github/workflows/retrain.yml` - Automated retraining

### Configuration

Edit `config/monitoring_config.json`:

```json
{
  "drift_threshold": 0.5,           // Drift detection threshold
  "performance_threshold": 0.75,     // Minimum acceptable accuracy
  "retraining_cooldown_hours": 24,  // Hours between retraining
  "enable_auto_retrain": true        // Enable automated retraining
}
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test
pytest tests/test_api.py -v

# With coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ”„ CI/CD Pipeline

### Continuous Integration (`.github/workflows/ci.yml`)
- Runs on every push/PR
- Linting, testing, security checks
- Multi-version Python testing

### Retraining & Deployment (`.github/workflows/retrain_deploy.yml`)
- Scheduled weekly retraining
- Automated model deployment
- Docker image building and pushing

## ğŸ“ Available Commands

```bash
make install          # Install dependencies
make setup           # Create directories
make download-data   # Download dataset
make train           # Train model
make serve           # Start API server
make mlflow-ui       # Start MLflow UI
make test            # Run tests
make format          # Format code
make lint            # Lint code
make clean           # Clean artifacts
make docker-build    # Build Docker image
make docker-run      # Run Docker container
```

## ğŸ”§ Development

### Add New Model

1. Add configuration in `config/model_config.yaml`
2. Update `src/models/train.py` to support new model
3. Test and train

### Add New Features

1. Modify `src/data/preprocessing.py`
2. Update configuration
3. Retrain model

## ğŸ“Š MLflow Tracking

All experiments are tracked in MLflow:
- Parameters (hyperparameters)
- Metrics (MAE, RMSE, RÂ², MAPE)
- Artifacts (models, plots, preprocessors)
- Models (versioned and registered)

Access MLflow UI at `http://localhost:5000`

## ğŸ” Environment Variables

```bash
MLFLOW_TRACKING_URI=./mlflow
PYTHONUNBUFFERED=1
```

## ğŸ“š Tech Stack

- **ML Framework**: scikit-learn, TensorFlow
- **Experiment Tracking**: MLflow
- **API Framework**: FastAPI
- **Data Processing**: Pandas, NumPy
- **Monitoring**: Evidently AI
- **Testing**: pytest
- **CI/CD**: GitHub Actions
- **Containerization**: Docker
- **Code Quality**: black, flake8, pylint

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

[Agasthya R Kumar](https://github.com/agasthyarkumar)

## ğŸ™ Acknowledgments

- California Housing Dataset from scikit-learn
- MLflow for experiment tracking
- FastAPI for the amazing web framework
- Evidently AI for drift detection

## ğŸ“® Contact

For questions or feedback, please open an issue on GitHub.

---

**Happy Predicting! ğŸ ğŸ“ˆ**
